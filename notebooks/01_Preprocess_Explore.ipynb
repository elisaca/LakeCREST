{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56004dfe",
   "metadata": {},
   "source": [
    "# LakeCREST ESA CCI Lakes v1.1\n",
    "## 1. Preprocessing\n",
    "In this script we will use [**xarray**](https://docs.xarray.dev/en/latest/index.html) and [**dask**](https://dask.org/) to load, prepare and explore the [**ESA CCI Lakes v1.1**](https://catalogue.ceda.ac.uk/uuid/ef1627f523764eae8bbb6b81bf1f7a0a) dataset. Using xarray and dask, two free and open-source libraries, allows us to fully utilize the computing power of our machine in parallelized workflows scaled to the system at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d2cb37",
   "metadata": {},
   "source": [
    "### 1.1 Importing modules\n",
    "First, we import the necessary python modules for the preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a764d406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from dask.distributed import Client, LocalCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb01a50",
   "metadata": {},
   "source": [
    "### 1.2 User inputs\n",
    "Here we define the desired lake and data paths. More information on available variables and a full list of all lake IDs can be found at the end of the [**D4.3: Product User Guide (PUG)**](https://climate.esa.int/media/documents/CCI-LAKES-0029-PUG_v1.1_signed_CA.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fc4b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in table of lakes with lake coordinates and the CCI_lakeid\n",
    "# based on D4.3 Product User Guide (PUG) - Annex B: List of lakes\n",
    "df = pd.read_csv('lakelist_v1.1.csv', delimiter=';')\n",
    "\n",
    "# Define lake\n",
    "lakename = 'Garda'\n",
    "\n",
    "# Find lakeid for specified lake\n",
    "lakeid = df.loc[df['name'] == lakename]['cci_lakeid'].values[0]\n",
    "print(f'The lakeid for Lake {lakename} is {lakeid}.')\n",
    "\n",
    "# Get a preview of table\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52422ab9",
   "metadata": {},
   "source": [
    "We can use [**hvplot**](https://hvplot.holoviz.org/) to plot the pandas table in a map-overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7ac1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.pandas\n",
    "\n",
    "# Plot pandas table to map-view with hvplot\n",
    "df.hvplot.points(x='longitude', y='latitude', \n",
    "                 color='red', alpha=0.5,\n",
    "                 geo=True, tiles='OSM', \n",
    "                 hover_cols='all',\n",
    "                 xlabel='Longitude', ylabel='Latitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3081b709",
   "metadata": {},
   "source": [
    "The paths are initialized as python raw-strings to avoid errors due to escape-sequences and then converted to pathlib.Path objects. The pathlib library allows to use paths within different operating systems and gives us access to many powerful functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c589c5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directory path and filenames\n",
    "path_data = r'D:\\lakecrest\\esa_cci_lp\\v1.1' # CCI data folder path\n",
    "path_mask = r'D:\\lakecrest\\esa_cci_lp\\mask\\ESACCI-LAKES_mask_v1.nc' # mask path\n",
    "path_dask = r'C:\\Users\\Micha\\Desktop\\dask' # Temporary dask workerspace\n",
    "\n",
    "# Get filepaths and convert to pathlib.Path\n",
    "path_data = pathlib.Path(path_data)\n",
    "path_mask = pathlib.Path(path_mask)\n",
    "path_dask = pathlib.Path(path_dask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be84f200",
   "metadata": {},
   "source": [
    "### 1.3 Dask initialization\n",
    "We initialize a local Dask client with our specified number of workers, threads per worker and memory limit (per worker). Calling the client outputs the client adress, so we can access the client over its webinterface. A good starting point for the settings is to set *n_workers* to the number of physical cores and *threads_per_worker* to number of virtual cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ec522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define according to system specs\n",
    "n_workers = 8              # (e.g. number of physical cores)\n",
    "threads_per_worker = 1     # (e.g. virtual cores / n_workers)\n",
    "memory_limit = '4GB'       # (e.g. max memory / n_workers)\n",
    "\n",
    "local_directory = path_dask\n",
    "cluster = LocalCluster(n_workers=n_workers, \n",
    "                       threads_per_worker=threads_per_worker, \n",
    "                       memory_limit=memory_limit,\n",
    "                       local_directory=local_directory\n",
    "                      )\n",
    "client = Client(address=cluster.scheduler_address)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c42490",
   "metadata": {},
   "source": [
    "### 1.4 Specify chunk size\n",
    "We use xarray to load the large multi-file dataset. xarray allows us to initialize and load the entire dataset by only providing the necessary filepaths. Instead of loading the entire dataset (>350GB) to memory, we can make use of xarray's ability to lazy-load chunks of data. This means that only the necessary subset of each individual .nc file will be loaded into memory at the time it is needed. For this we can either pre-define a chunk size or let xarray automatically define a size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0911ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set chunk size based on .nc file dims and a divider\n",
    "#input_lat_sz = 21600\n",
    "#input_lon_sz = 43200\n",
    "#divider=100\n",
    "\n",
    "#chunk_lat_sz = int(input_lat_sz/divider)\n",
    "#chunk_lon_sz = int(input_lon_sz/divider)\n",
    "#chunks={'lat':chunk_lat_sz,\n",
    "#        'lon':chunk_lon_sz,\n",
    "#        'time':1\n",
    "#        }\n",
    "\n",
    "# Alternatively set chunks to 'auto' (dask decides chunk size)\n",
    "chunks='auto'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d885faf5",
   "metadata": {},
   "source": [
    "### 1.5 Load an individual .nc file\n",
    "To test out xarray and get a preview of the ESA CCI Lakes dataset we can load a single file from the dataset. For this we will use the [xarray.open_dataset](https://docs.xarray.dev/en/stable/generated/xarray.open_dataset.html) function. We can get a preview of the loaded data, its attributes and variables in the console view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de11abd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pathlib.Path.rglob function to recursively find all .nc files within the data folder\n",
    "paths_data = list(path_data.rglob('*fv1.1.nc'))\n",
    "\n",
    "# Get the first filepath from the list of .nc files\n",
    "path_fn_first = paths_data[0]\n",
    "\n",
    "# Load the file with xarray\n",
    "DS_preview = xr.open_dataset(filename_or_obj=path_fn_first,\n",
    "                             engine='netcdf4',\n",
    "                             chunks=chunks)\n",
    "DS_preview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920c5571",
   "metadata": {},
   "source": [
    "### 1.5 Spatial subsetting and variable selection\n",
    "Because we are only interested in a specific lake and a subset of the 50+ available variables we subset the dataset. Therefore, we setup a function **preprocess(ds)** that subsets the daily .nc file to only fetch information about the desired variables in a bounding-box based on the bounding coordinates of the desired lake. The preprocess function will be run on every daily .nc file when the multifile-dataset is initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d985622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the variables we want to load\n",
    "variables = ['lake_surface_water_temperature',\n",
    "              'lswt_quality_level',\n",
    "              'lswt_uncertainty'\n",
    "              ]\n",
    "\n",
    "# Load CCI lakes mask\n",
    "DS_mask = xr.open_dataset(filename_or_obj=path_mask,\n",
    "                          engine='netcdf4',\n",
    "                          decode_cf=True,\n",
    "                          chunks=chunks\n",
    "                          )\n",
    "\n",
    "# Get logical True/False lake mask over full globe\n",
    "mask_full = (DS_mask.CCI_lakeid == lakeid)\n",
    "\n",
    "# Get logical True/False lake mask sliced over ROI only\n",
    "mask_roi = DS_mask.CCI_lakeid.where(mask_full, \n",
    "                                   drop=True)\n",
    "\n",
    "# Get bounds coordinates\n",
    "lat_min = mask_roi.lat[0].values\n",
    "lat_max = mask_roi.lat[-1].values\n",
    "lon_min = mask_roi.lon[0].values\n",
    "lon_max = mask_roi.lon[-1].values\n",
    "\n",
    "# Subset the lake mask to same coords\n",
    "#DS_mask_roi = DS_mask.sel(lat=slice(lat_min, lat_max), \n",
    "#                          lon=slice(lon_min, lon_max))\n",
    "\n",
    "def preprocess(ds):\n",
    "    '''Keeps only the necessary lat/lon slice and necessary variables when opening .nc files'''\n",
    "    return ds[variables].sel(lat=slice(lat_min, lat_max), \n",
    "                             lon=slice(lon_min, lon_max))\n",
    "\n",
    "print(f'Created preprocess(ds) function to subset with bbox ' \\\n",
    "      f'lat: ({lat_min:0.1f}, {lat_max:0.1f}), ' \\\n",
    "      f'lon: ({lon_min:0.1f}, {lon_max:0.1f}) for Lake {lakename}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e33c0a4",
   "metadata": {},
   "source": [
    "We can check the lake mask and the computed bounding box based on the mask file on a mapview using [**hvplot**](https://hvplot.holoviz.org/) and [**GeoViews**](https://geoviews.org/). Both objects are based on the [**HoloViews**](https://holoviews.org/) library and can be easily combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827016e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.xarray\n",
    "import geoviews as gv\n",
    "\n",
    "# Create map-plot of xarray dataarray using hvplot\n",
    "hv_map = mask_roi.hvplot(geo=True, tiles='CartoLight', colorbar=False, \n",
    "                         xlabel='Longitude', ylabel='Latitude')\n",
    "\n",
    "# Create boundingbox using geoviews\n",
    "gv_bbox = gv.Rectangles([(lon_min, lat_min, lon_max, lat_max)]).opts(color='none', line_width=2, line_color='red')\n",
    "\n",
    "# Combine objects as overlay\n",
    "hv_map * gv_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202ff3a6",
   "metadata": {},
   "source": [
    "### 1.6 Load full dataset as xarray.Dataset\n",
    "Now, we can initialize the full dataset using xarray's [**xarray.open_mfdataset**](https://docs.xarray.dev/en/latest/generated/xarray.open_mfdataset.html) function. We will set ***decode_cf*** to **false** for now and decode the dataset later. During the loading process we can monitor the progress and the task stream of our workers in the dask webinterface (output from *1.3 Dask initialization*).\n",
    "\n",
    "The xarray documentation has an extensive [user-guide](https://xarray.pydata.org/en/stable/user-guide/io.html) with explanations and best-practices to load large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b88df3a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Setup timer to time the loading process\n",
    "start_time = time.time()\n",
    "\n",
    "DS = xr.open_mfdataset(paths=paths_data,\n",
    "                       combine='by_coords',\n",
    "                       parallel=True,\n",
    "                       engine='netcdf4',\n",
    "                       decode_cf=False,\n",
    "                       preprocess=preprocess,\n",
    "                       chunks=chunks\n",
    "                       )\n",
    "\n",
    "print(f'Xarray dataset with variables: {variables} initialized after ' \\\n",
    "      f'{(time.time()-start_time):0.1f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5537d7ea",
   "metadata": {},
   "source": [
    "Once the dataset has been loaded, we can get a overview of the xarray.Dataset object and its variables and attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db624099",
   "metadata": {},
   "outputs": [],
   "source": [
    "DS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6421d43",
   "metadata": {},
   "source": [
    "### 1.7 Apply lake-mask\n",
    "Next we apply the lake mask to mask cells of possible other lakes in the same bounding box. To make sure that the lake mask has identical cell coordinates we align it to the coordinates of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1291e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reindex coordinates to make sure that our lake mask cells are aligned to data cells\n",
    "da_mask = mask_roi.reindex_like(other=DS, method='nearest')\n",
    "\n",
    "# Get subset with lakemask and xarray.DataArray.where\n",
    "DS_clip = DS.where(cond=da_mask, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e73e080",
   "metadata": {},
   "source": [
    "### 1.8 Decode the data\n",
    "xarray will handle the data-decoding of the NetCDF format with the scaling- and offset-attributes found in the loaded files. We can use [xarray.decode_cf](https://docs.xarray.dev/en/latest/generated/xarray.decode_cf.html) to automatically decode the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290b9da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode data\n",
    "DS_clip = xr.decode_cf(DS_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d72967d",
   "metadata": {},
   "source": [
    "### 1.9 Drop days with no data\n",
    "We can drop days with all-nan values for the variable 'lake_surface_water_temperature' by using the [**xarray.Dataset.dropna**](https://docs.xarray.dev/en/latest/generated/xarray.Dataset.dropna.html) function with the parameter ***how='all'***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d32c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_clip_dropna = DS_clip.dropna(dim='time', \n",
    "                                how='all',\n",
    "                                subset=['lake_surface_water_temperature'])\n",
    "\n",
    "dropcount = DS_clip.time.size - DS_clip_dropna.time.size\n",
    "print(f'Dropped {dropcount} days with all-nan cells (from {DS_clip.time.size} to {DS_clip_dropna.time.size})')\n",
    "\n",
    "DS_clip_dropna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7194c936",
   "metadata": {},
   "source": [
    "### 1.10 Mask LSWT with quality flag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249cacd5",
   "metadata": {},
   "source": [
    "Using the variable 'lswt_quality_level' we can mask out cells that don't fulfill the necessary quality criteria that ranges from 0 to 5:\n",
    "\n",
    "- 0: unprocessed \n",
    "- 1: bad \n",
    "- 2: marginal \n",
    "- 3: intermediate\n",
    "- 4: good \n",
    "- 5: best\n",
    "\n",
    "Within the [D4.1: Product Validation and Intercomparison Report](https://climate.esa.int/media/documents/CCI-LAKES-0031-PVIR_v1.4.pdf) the authors of the ESA CCI Lakes product recommend to limit the use of LSWT for lake-climate applications to quality flags 4-5 (good and best)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1ec035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use quality flag variable 'lswt_quality_level' to mask out all cells except good and best\n",
    "qmask = (DS_clip_dropna['lswt_quality_level']>3)\n",
    "DS_clip_dropna['lake_surface_water_temperature'] = DS_clip_dropna['lake_surface_water_temperature'].where(cond=qmask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ee2045",
   "metadata": {},
   "source": [
    "### 1.11 Export subset (optional)\n",
    "If necessary, the subset with the masked data can now be exported using [**xarray.Dataset.to_netcdf**](https://xarray.pydata.org/en/stable/generated/xarray.Dataset.to_netcdf.html). We can get the encoding settings (e.g. compression, fillvalue, scale-factor, offset) for the NetCDF export from the previously loaded dataset. Instead of exporting all data we can also export a temporal subset by slicing the data in time. Exporting the data to NetCDF format is slow, since the data is first loaded, decompressed and decoded and then encoded and compressed again for storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b44b38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_dst = f'D:\\lakecrest\\output\\ESACCI-LAKES-L3S-LK_PRODUCTS-MERGED-{lakename}-fv2.0.nc'\n",
    "path_dst = pathlib.Path(path_dst)\n",
    "\n",
    "# Get encoding settings from DS.encoding and DS.attrs to ensure the encoding setting are identical\n",
    "DS_enc = {}\n",
    "encoding_enc = ['zlib', 'shuffle', 'complevel', \n",
    "              'fletcher32', 'contiguous', 'dtype']\n",
    "attr_enc = ['_FillValue', 'scale_factor', 'add_offset']\n",
    "for var in variables:\n",
    "    DS_enc_encoding = DS.get(var).encoding\n",
    "    DS_enc_fromenc = {k:v for k, v in DS_enc_encoding.items() \\\n",
    "                      if k in encoding_enc}\n",
    "    DS_enc_attrs = DS.get(var).attrs\n",
    "    DS_enc_fromattrs = {k:v for k, v in DS_enc_attrs.items() \\\n",
    "                        if (k in attr_enc and hasattr(DS.get(var), k))}\n",
    "    DS_enc[var] = {**DS_enc_fromenc, **DS_enc_fromattrs}\n",
    "print('DS encoding:', DS_enc)\n",
    "\n",
    "# Set starting time for timer\n",
    "start_time = time.time() \n",
    "\n",
    "# We can also slice the data in time before exporting\n",
    "#timeslice = slice('2019-01-01', '2020-01-01)                 \n",
    "#DS_clip_dropna = lswt_celsius.sel(time=timeslice)\n",
    "\n",
    "DS_clip_dropna.to_netcdf(path=path_dst,\n",
    "                         mode='w',\n",
    "                         engine='netcdf4',\n",
    "                         encoding=DS_enc\n",
    "                        )\n",
    "\n",
    "print(f'ESA CCI Lakes v2.0 subset of Lake {lakename} ' \\\n",
    "      f'exported after {(time.time()-start_time):0.1f} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcdd2a4",
   "metadata": {},
   "source": [
    "Now we can load our entire lakedata from a single .nc files instead of the 9000+ daily files of the dataset. This will improve the efficiency for future computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c479ba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassign dataset variable with our single-file based subset\n",
    "DS_clip_dropna = xr.open_dataset(filename_or_obj=path_dst,\n",
    "                                 combine='by_coords',\n",
    "                                 parallel=True,\n",
    "                                 engine='netcdf4',\n",
    "                                 decode_cf=False\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db29714",
   "metadata": {},
   "source": [
    "## 2. Explore dataset\n",
    "### 2.1 LSWT timeseries animation\n",
    "Let's explore the time-series LSWT data. To do this we first convert the [xarray.Dataset](https://docs.xarray.dev/en/latest/generated/xarray.Dataset.html) to a [xarray.Dataarray](https://docs.xarray.dev/en/latest/generated/xarray.DataArray.html) containing only the variable 'lake_surface_water_temperature' and convert the value from °K to °C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736a5019",
   "metadata": {},
   "outputs": [],
   "source": [
    "lswt = DS_clip_dropna['lake_surface_water_temperature']-273.15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadd9f15",
   "metadata": {},
   "source": [
    "Now we can create an interactive animation that displays the temperature data in a cartographic reference system using [Holoviews](https://holoviews.org/) and [Cartopy](https://scitools.org.uk/cartopy/docs/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a387c092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.xarray\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "# Define CRS\n",
    "crs = ccrs.PlateCarree(central_longitude=0, globe=None)\n",
    "\n",
    "# Plot time-series data\n",
    "lswt.hvplot(\n",
    "    geo=True, tiles='CartoLight',\n",
    "    groupby=\"time\",  # adds a widget for time\n",
    "    clim=(0, 25),  # sets colormap limits\n",
    "    crs=crs,\n",
    "    cmap='jet',\n",
    "    widget_type=\"scrubber\",\n",
    "    widget_location=\"bottom\",\n",
    "    #width=300,\n",
    "    xlabel='Longitude', ylabel='Latitude',\n",
    "    #title=f'ESA CCI Lakes v1.1\\nLake {lakename} LSWT animation',\n",
    "    clabel='Lake surface water temperature (°C)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a8eafa",
   "metadata": {},
   "source": [
    "### 2.2 Mean LSWT plot\n",
    "We can use [**xarray.DataArray.mean**](https://docs.xarray.dev/en/latest/generated/xarray.DataArray.mean.html) with the parameters ***dim*** and ***skipna*** to plot the spatially aggregated daily LSWT mean over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3457f341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.xarray\n",
    "\n",
    "# Slice data with timeslice\n",
    "timeslice = slice('2006-01-01', '2007-12-31')\n",
    "lswt_slice = lswt.sel(time=timeslice)\n",
    "\n",
    "# Compute daily means\n",
    "lswt_slice_mean = lswt_slice.mean(dim=[\"lat\", \"lon\"], skipna=True).load()\n",
    "\n",
    "# Create a line plot using hvplot\n",
    "lineplot = lswt_slice_mean.hvplot.line(ylabel='LSWT (°C)', \n",
    "                                       title=f'ESA CCI Lakes v1.1\\nLake {lakename} LSWT mean')\n",
    "\n",
    "# Create a scatter plot using hvplot\n",
    "scatterplot = lswt_slice_mean.hvplot.scatter(c='k')\n",
    "\n",
    "# Overlay plots to create output\n",
    "lineplot * scatterplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8133fb6e",
   "metadata": {},
   "source": [
    "As we can see there are days with outliers. We can set a treshold for a minimal lake coverage and replot the data. For this we can count the unmasked cells in our lake mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a6cbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup treshold for minimal lake coverage\n",
    "coverage_tresh = 20 # treshold in %\n",
    "\n",
    "# Calculate cell treshold to keep daily mean\n",
    "lakecells = np.count_nonzero(~np.isnan(mask_roi)) # count of lakecells\n",
    "nancells_tresh = int(lakecells*((coverage_tresh)/100)) # minimal cells treshold\n",
    "print(f'Dropping days with less than {nancells_tresh} valid cells ({coverage_tresh}% of Lake {lakename}).')\n",
    "\n",
    "# Use xarray.DataArray.dropna with threshold to get rid of days with low-coverage\n",
    "lswt_slice_tresh = lswt_slice.dropna(dim='time', thresh=nancells_tresh)\n",
    "\n",
    "# Compute daily means\n",
    "lswt_slice_tresh_mean = lswt_slice_tresh.mean(dim=[\"lat\", \"lon\"], skipna=True)\n",
    "\n",
    "# Create plot\n",
    "lineplot = lswt_slice_tresh_mean.hvplot.line(ylabel='LSWT (°C)', \n",
    "                                       title=f'ESA CCI Lakes v1.1\\nLake {lakename} LSWT mean (>20% coverage)')\n",
    "\n",
    "scatterplot = lswt_slice_tresh_mean.hvplot.scatter(c='k')\n",
    "\n",
    "print(f'Dropped {lswt_slice_mean.size - lswt_slice_tresh_mean.size} datapoints.')\n",
    "\n",
    "lineplot * scatterplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d7aaee",
   "metadata": {},
   "source": [
    "### 2.3 Visualize data availability\n",
    "As we can see in the mean plots the data availability changes in accordance with missions timeline of data sources. We can use the time dimension to calculate the yearly mean coverage frequency per cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1d271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "# Setup function to compute mean time-delay of non-nan values along\n",
    "def getCovFreq(darr):\n",
    "    \"\"\"Takes xarray.DataArray and returns dataarray with mean timedelay (d) between valid values\"\"\"\n",
    "    lat_sz, lon_sz = darr.lat.size, darr.lon.size # get lat and lon sizes\n",
    "    mask = darr.to_masked_array() # create a np.masked_array from xr.dataarray\n",
    "\n",
    "    t = darr.time.values # get time dimension as array\n",
    "    t_steps = len(darr.time)\n",
    "\n",
    "    arr_t3d = np.broadcast_to(t[:, np.newaxis, np.newaxis], (t_steps, lat_sz, lon_sz)) # expand time dimension to match dataarray\n",
    "    arr_t3d_masked = np.ma.masked_where(mask.mask, arr_t3d).filled(np.datetime64('NaT'))  # mask it and set masked cells to nan\n",
    "    darr_t3d_masked = da.from_array(arr_t3d_masked, chunks='auto') # convert np.arr to dask-array\n",
    "\n",
    "    def mean_cfreq(x):\n",
    "        \"\"\"Takes np.datetime64 array and returns np.float32 array with temp-diff (d), dropping nat-values\"\"\"\n",
    "        arr = x[~np.isnat(x)] # drop nats\n",
    "        diff_td64 = np.diff(arr)\n",
    "        diff_d = (diff_td64 / np.timedelta64(1,'D')).astype(np.float32)\n",
    "        return np.mean(diff_d) # return mean of differences\n",
    "\n",
    "    # Apply mean_cfreq along time-dimension\n",
    "    cfreq = np.apply_along_axis(mean_cfreq, 0, arr_t3d_masked)\n",
    "    xr_cfreq = xr.DataArray(data=cfreq, coords=(darr.lat.values, darr.lon.values), dims=('lat', 'lon'), name='mean_cfreq')\n",
    "    return(xr_cfreq)\n",
    "\n",
    "# Slice data with timeslice\n",
    "timeslice = slice('2005-01-01', '2008-12-31')\n",
    "lswt_slice = lswt.sel(time=timeslice)\n",
    "\n",
    "# Group LSWT by years and compute the yearly mean cell coverage frequency\n",
    "lswt_cfrwq = lswt_slice.groupby(\"time.year\").map(getCovFreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e78bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a violin plot of the distribution of yearly mean coverage frequency\n",
    "lswt_cfrwq.hvplot.violin(y='mean_cfreq', by='year', \n",
    "                         title=f'ESA CCI Lakes v1.1\\nLake {lakename} - Coverage frequency', \n",
    "                         ylabel='Mean yearly coverage frequency (d)', \n",
    "                         grid=True, \n",
    "                         ylim=[0,60],\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e4c9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot yearly mean coverage frequency maps \n",
    "lswt_cfrwq.hvplot(geo=True, tiles='CartoLight',\n",
    "                           groupby=\"year\",  # adds a widget for time\n",
    "                           clim=(0, 25),  # sets colormap limits\n",
    "                           crs=crs,\n",
    "                           cmap='jet',\n",
    "                           widget_type=\"scrubber\",\n",
    "                           widget_location=\"bottom\",\n",
    "                           #width=300,\n",
    "                           xlabel='Longitude', ylabel='Latitude',\n",
    "                           #title=f'ESA CCI Lakes v1.1\\nLake {lakename} LSWT coverage frequency',\n",
    "                           clabel='mean coverage frequency (d)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48989d9b",
   "metadata": {},
   "source": [
    "### 2.4 Linear trend analysis\n",
    "We can compute the linear trend for each lake cell using the [xarray.DataArray.polyfit](https://docs.xarray.dev/en/stable/generated/xarray.DataArray.polyfit.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cc77a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "polyfit_results = lswt.polyfit(dim='time', deg=1, skipna=True)\n",
    "\n",
    "ns_per_year = 3.154e+16\n",
    "ns_per_decade = ns_per_year*10\n",
    "trend_coef = polyfit_results.polyfit_coefficients.sel(degree=1)\n",
    "trend_coef_perDec = trend_coef*ns_per_decade.persist() # convert °C/ns to °C/10y and load to memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66afaa9",
   "metadata": {},
   "source": [
    "Plot the linear coefficients on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db110ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "vmax = np.nanpercentile(trend_coef_perDec, 95)\n",
    "vmax = math.ceil(vmax*10)/10\n",
    "\n",
    "trend_coef_perDec.hvplot(\n",
    "    clabel='trend (°C/10y)', \n",
    "    label=f'Lake {lakename}, Linear trend',\n",
    "    geo=True, \n",
    "    tiles='StamenTerrainRetina', # plot backgroundmap\n",
    "    #features={'rivers':'10m'},\n",
    "    cmap='bwr',\n",
    "    clim=(-vmax, vmax),\n",
    "    crs=crs,\n",
    "    #width=300\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933ea6b5",
   "metadata": {},
   "source": [
    "Calculate the mean trend (°C/10y) over entire lake by applying the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cd682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmean(trend_coef_perDec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
