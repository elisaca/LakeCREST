{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b6862af",
   "metadata": {},
   "source": [
    "# LakeCREST notebooks\n",
    "*Version: 08.04.2022 14:45*\n",
    "## 1. Unpack ESA CCI Lakes (multiple lakes, from disk)\n",
    "In this script we will use [**xarray**](https://docs.xarray.dev/en/latest/index.html) and [**dask**](https://dask.org/) to load, mask and subset the [**ESA CCI Lakes v1.1**](https://catalogue.ceda.ac.uk/uuid/ef1627f523764eae8bbb6b81bf1f7a0a). The unpacking in this script is based on loading the pre-downloaded dataset from a local disk. Using xarray and dask, two free and open-source libraries, allows us to fully utilize the computing power of our machine in parallelized workflows scaled to the system at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d2cb37",
   "metadata": {},
   "source": [
    "### 1.1 Importing modules\n",
    "First, we import the necessary python modules for the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a764d406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from dask.distributed import Client, LocalCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb01a50",
   "metadata": {},
   "source": [
    "### 1.2 Define ROI\n",
    "Here we define the desired lake and data paths. These are the only necessary user inputs to run the export of LSWT+LIC subsets. Depending on the system the dask settings in *1.3 Dask initialization* can be adapted as well to confine to individual memory and processing limits. To export data on other available variables, more information can be found in the [**D4.3: Product User Guide (PUG)**](https://climate.esa.int/media/documents/CCI-LAKES-0029-PUG_v1.1_signed_CA.pdf). To mask the lakes from the global dataset we are using the lakemask \"ESACCI-LAKES_mask_v1.nc\" that can be accessed as part of [**ESA CCI Lakes v1.0**](https://catalogue.ceda.ac.uk/uuid/3c324bb4ee394d0d876fe2e1db217378).\n",
    "#### Define filepaths (âœ¦ User inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdab7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lake to unpack\n",
    "lakenames = ['Ontario', 'Huron', 'Michigan', 'Kariba', 'Garda']\n",
    "\n",
    "# Define data directory path and filenames\n",
    "path_data = pathlib.Path(r'D:\\lakecrest\\esa_cci_lp\\v1.1') # Path to ESA CCI Lakes data folder path\n",
    "path_mask = pathlib.Path(r'D:\\lakecrest\\esa_cci_lp\\mask\\ESACCI-LAKES_mask_v1.nc') # Path to lakemask\n",
    "path_dask = pathlib.Path(r'C:\\Users\\Micha\\Desktop\\dask') # Path to temporary dask workerspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5156f1ef",
   "metadata": {},
   "source": [
    "Now we can search for the CCI_lakeid corresponding to the defined lakes using the provided table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fc4b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in table of lakes with lake coordinates and the CCI_lakeid\n",
    "# based on D4.3 Product User Guide (PUG) - Annex B: List of lakes\n",
    "try:\n",
    "    df = pd.read_csv('lakelist_v1.1.csv', delimiter=';')\n",
    "except:\n",
    "    print('Error: Did not find the lakelist .csv file, check that it is in the same folder!')\n",
    "\n",
    "for lake in lakenames:\n",
    "    if not(lake in list(df['name'])):\n",
    "        print(f'Warning: Lake {lake} was not found, check spelling!')\n",
    "\n",
    "# Get Preview of selected lakes\n",
    "df[df['name'].isin(lakenames)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be84f200",
   "metadata": {},
   "source": [
    "### 1.3 Dask initialization\n",
    "We initialize a local Dask client with our specified number of workers, threads per worker and memory limit (per worker). Calling the client outputs the client adress, so we can access the client over its webinterface. A good starting point for the settings is to set *n_workers* to the number of physical cores and *threads_per_worker* to number of virtual cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ec522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define according to system specs\n",
    "n_workers = 2            # (e.g. number of physical cores)\n",
    "threads_per_worker = 8     # (e.g. virtual cores / n_workers)\n",
    "memory_limit = '16GB'       # (e.g. max memory / n_workers)\n",
    "\n",
    "local_directory = path_dask\n",
    "cluster = LocalCluster(n_workers=n_workers, \n",
    "                       threads_per_worker=threads_per_worker, \n",
    "                       memory_limit=memory_limit,\n",
    "                       local_directory=local_directory\n",
    "                      )\n",
    "client = Client(address=cluster.scheduler_address)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c42490",
   "metadata": {},
   "source": [
    "### 1.4 Specify chunk size\n",
    "We use xarray to load the large multi-file dataset. xarray allows us to initialize and load the entire dataset by only providing the necessary filepaths. Instead of loading the entire dataset (>350GB) to memory, we can make use of xarray's ability to lazy-load chunks of data. This means that only the necessary subset of each individual .nc file will be loaded into memory at the time it is needed. For this we can either pre-define a chunk size or let xarray automatically define a size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0911ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dask decides chunk size\n",
    "chunks='auto'\n",
    "\n",
    "# Alternatively set chunks to specified size\n",
    "# chunks={'lat':10,\n",
    "#         'lon':10,\n",
    "#         'time':1\n",
    "#         }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920c5571",
   "metadata": {},
   "source": [
    "### 1.5 Spatial subsetting\n",
    "Because we are only interested in a specific lakes we define bounding boxes for each the desired lake based on the provided lake mask file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d985622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CCI lakes mask\n",
    "DS_mask = xr.open_dataset(filename_or_obj=path_mask,\n",
    "                          engine='netcdf4',\n",
    "                          decode_cf=True,\n",
    "                          chunks=chunks\n",
    "                          )\n",
    "lakedict = {}\n",
    "\n",
    "for lake in lakenames:\n",
    "    # Get lakeid\n",
    "    lakeid = lakeid = df.loc[df['name'] == lake]['cci_lakeid'].values[0]\n",
    "    \n",
    "    # Get logical True/False lake mask over full globe\n",
    "    mask_full = (DS_mask.CCI_lakeid == lakeid)\n",
    "\n",
    "    # Get logical True/False lake mask sliced over ROI only\n",
    "    mask_roi = mask_full.where(mask_full, drop=True)\n",
    "\n",
    "    # Get bounds coordinates\n",
    "    lat_min = mask_roi.lat[0].values\n",
    "    lat_max = mask_roi.lat[-1].values\n",
    "    lon_min = mask_roi.lon[0].values\n",
    "    lon_max = mask_roi.lon[-1].values\n",
    "    \n",
    "    lakedict[lake] = {\n",
    "        'name': lake,\n",
    "        'id': lakeid,\n",
    "        'lat_min': lat_min,\n",
    "        'lat_max': lat_max,\n",
    "        'lon_min': lon_min,\n",
    "        'lon_max': lon_max,\n",
    "        'mask_roi': mask_roi}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e33c0a4",
   "metadata": {},
   "source": [
    "We can check the lake masks and the computed bounding boxes based on the mask file on a mapview using [**hvplot**](https://hvplot.holoviz.org/) and [**GeoViews**](https://geoviews.org/). Both objects are based on the [**HoloViews**](https://holoviews.org/) library and can be easily combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827016e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.xarray\n",
    "import geoviews as gv\n",
    "import holoviews as hv\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings about unevenly sampled axes, \n",
    "# this is expected because we are working with geographic coordinates\n",
    "warnings.filterwarnings(\"ignore\", message=\"Image dimension.*\")\n",
    "\n",
    "plots = []\n",
    "\n",
    "for lake in lakedict.values():\n",
    "    # Create mapplot of mask\n",
    "    hv_mask = lake['mask_roi'].hvplot(geo=True, tiles='CartoLight', colorbar=False, \n",
    "                              xlabel='Longitude', ylabel='Latitude', title=f'Lake {lake[\"name\"]}',\n",
    "                                     width=300, height=300)\n",
    "    # Create mapplot of bounding box\n",
    "    gv_bbox = gv.Rectangles([(lake['lon_min'], lake['lat_min'], \n",
    "                              lake['lon_max'], lake['lat_max'])])\\\n",
    "    .opts(color='none', line_width=2, line_color='red')\n",
    "    # Combine mapplots and add to list of plots\n",
    "    plots.append(hv_mask*gv_bbox)\n",
    "\n",
    "# Plot ROIs together\n",
    "combined = hv.Layout(plots).cols(3)\n",
    "combined.opts(shared_axes=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d885faf5",
   "metadata": {},
   "source": [
    "### 1.6 Load an individual .nc file\n",
    "To test out xarray and get a preview of the ESA CCI Lakes dataset we can load a single file from the dataset. For this we will use the [xarray.open_dataset](https://docs.xarray.dev/en/stable/generated/xarray.open_dataset.html) function. We can get a preview of the loaded data, its attributes and variables in the console view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de11abd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use pathlib.Path.rglob function to recursively find all .nc files within the data folder\n",
    "paths_data = list(path_data.rglob('*fv1.1.nc'))\n",
    "\n",
    "# Get the first filepath from the list of .nc files\n",
    "path_fn_first = paths_data[0]\n",
    "\n",
    "# Load the file with xarray\n",
    "DS_preview = xr.open_dataset(filename_or_obj=path_fn_first,\n",
    "                             engine='netcdf4',\n",
    "                             decode_cf=False,\n",
    "                             chunks=chunks)\n",
    "DS_preview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202ff3a6",
   "metadata": {},
   "source": [
    "### 1.7 Load full dataset as xarray.Dataset\n",
    "Now, we can initialize the full dataset using xarray's [**xarray.open_mfdataset**](https://docs.xarray.dev/en/latest/generated/xarray.open_mfdataset.html) function. Xarray will handle the data-decoding of the NetCDF format with the scaling- and offset-attributes found in the loaded files. During the loading process we can monitor the progress and the task stream of our workers in the dask webinterface (output from *1.3 Dask initialization*). Once the dataset is loaded, we'll get a preview.\n",
    "\n",
    "The xarray documentation has an extensive [user-guide](https://xarray.pydata.org/en/stable/user-guide/io.html) with explanations and best-practices to load large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b88df3a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define variables (rest is dropped from dataset)\n",
    "variables = ['lake_surface_water_temperature',\n",
    "              'lswt_quality_level',\n",
    "              'lswt_uncertainty',\n",
    "              'lake_ice_cover'\n",
    "              ]\n",
    "\n",
    "# Create preprocess function that drops unnecessary variables\n",
    "def preprocess(ds):\n",
    "    return(ds[variables])\n",
    "\n",
    "# Setup timer to time the loading process\n",
    "start_time = time.time()\n",
    "\n",
    "DS = xr.open_mfdataset(paths=paths_data,\n",
    "                       combine='by_coords',\n",
    "                       parallel=True,\n",
    "                       engine='netcdf4',\n",
    "                       decode_cf=True,\n",
    "                       chunks=chunks,\n",
    "                       preprocess=preprocess)\n",
    "\n",
    "print(f'Xarray dataset with variables: {variables} initialized after ' \\\n",
    "      f'{(time.time()-start_time):0.1f} seconds')\n",
    "\n",
    "DS # Load preview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ee2045",
   "metadata": {},
   "source": [
    "### 1.8 Export subsets\n",
    "The subset with the masked data can now be exported using [**xarray.Dataset.to_zarr**](https://xarray.pydata.org/en/stable/generated/xarray.Dataset.to_zarr.html). We can get the encoding settings (e.g. compression, fillvalue, scale-factor, offset) for the export from the previously loaded preview (*1.5 Load an individual .nc file*) of the dataset.\n",
    "\n",
    "We apply the lake mask to mask cells of other lakes in the same bounding box. To make sure that the lake mask has identical cell coordinates we first align it to the coordinates of our dataset. Now we can export the subset. Exporting the files to the [zarr](https://zarr.readthedocs.io/en/stable/)-format instead of NetCDF is more efficient in parralel-writing and compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b44b38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define output path in /subsets folder\n",
    "# Form pathlib objects\n",
    "path_wrk = pathlib.Path().absolute()\n",
    "path_ss = path_wrk.joinpath('subsets')\n",
    "# create subsets folder if it doesnt exit yet\n",
    "path_ss.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Get exports for each lake\n",
    "for lake in lakedict.values():\n",
    "    \n",
    "    ### Slice data, apply lakemask and subset variables\n",
    "    ### -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
    "    \n",
    "    #slice\n",
    "    DS_lake = DS.sel(lat=slice(lake['lat_min'], lake['lat_max']), lon=slice(lake['lon_min'], lake['lon_max']))\n",
    "    \n",
    "    # Get mask from dict\n",
    "    mask_roi = lake['mask_roi']\n",
    "    \n",
    "    # Reindex coordinates to make sure that our lake mask cells are aligned to data cells\n",
    "    da_mask = mask_roi.reindex_like(other=DS_lake, method='nearest')\n",
    "    # Load mask to memory\n",
    "    da_mask.load()\n",
    "\n",
    "    # Slice dataset to bbox and mask data\n",
    "    DS_lake_masked = DS_lake.where(cond=(da_mask==True))\n",
    "    \n",
    "    # Rechunk subset\n",
    "    lat_chunk = DS_lake_masked.lat.size\n",
    "    lon_chunk = DS_lake_masked.lon.size\n",
    "    DS_lake_masked = DS_lake_masked.chunk({'lat':lat_chunk, 'lon':lon_chunk, 'time':1})\n",
    "    DS_lake_masked\n",
    "\n",
    "    ### Run export for subset\n",
    "    ### -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
    "\n",
    "    import zarr\n",
    "    \n",
    "    # Define an output path for the subset file\n",
    "    lakename = lake[\"name\"]\n",
    "    path_dst = path_ss.joinpath(fr'ESACCI-LAKES-L3S-LK_PRODUCTS-MERGED-LSWT_LIC-{lakename}-fv1.1.zarr')\n",
    "\n",
    "    # Get encoding settings from un-decoded DS_preview\n",
    "    DS_enc = {}\n",
    "    encoding_enc = ['dtype'] # ,'zlib', 'shuffle', 'complevel', 'fletcher32', 'contiguous']\n",
    "    attr_enc = ['_FillValue', 'scale_factor', 'add_offset']\n",
    "    for var in variables:\n",
    "        DS_enc_encoding = DS_preview.get(var).encoding\n",
    "        DS_enc_fromenc = {k:v for k, v in DS_enc_encoding.items() \\\n",
    "                          if k in encoding_enc}\n",
    "        DS_enc_attrs = DS_preview.get(var).attrs\n",
    "        DS_enc_fromattrs = {k:v for k, v in DS_enc_attrs.items() \\\n",
    "                            if (k in attr_enc and hasattr(DS_preview.get(var), k))}\n",
    "        DS_enc[var] = {**DS_enc_fromenc, **DS_enc_fromattrs}\n",
    "\n",
    "    # Setup zarr-compressor using Blosc and zstd-compression\n",
    "    compressor = zarr.Blosc(cname=\"zstd\", clevel=3, shuffle=2)\n",
    "\n",
    "    # Set compression settings in encoding\n",
    "    for var in DS_enc.items():\n",
    "        var[1]['compressor'] = compressor\n",
    "\n",
    "    #print('DS encoding:')\n",
    "    #for var in DS_enc.items(): \n",
    "    #    print(var)\n",
    "\n",
    "    # Set starting time for timer\n",
    "    start_time = time.time() \n",
    "\n",
    "    print(f'Exporting for Lake {lakename} started.. ', end='')\n",
    "    DS_lake_masked.to_zarr(store=path_dst,\n",
    "                           mode='w',\n",
    "                           encoding=DS_enc)\n",
    "    print(f'Subset exported after {(time.time()-start_time):0.0f} seconds.')\n",
    "\n",
    "print('All subsets generated!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e491459d",
   "metadata": {},
   "source": [
    "#### Convert subset zarr-folders to zip-files\n",
    "To make the zarr-folders easier to handle we can put them in zip-archives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4471c004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "#shutil.make_archive(output_filename, 'zip', dir_name)\n",
    "paths_ss = list(path_ss.glob('*zarr'))\n",
    "for path in paths_ss:\n",
    "    # Set starting time for timer\n",
    "    start_time = time.time()\n",
    "    fn_dst = path.stem\n",
    "    path_dst = path_ss.joinpath(fn_dst)\n",
    "    # Create .zip if it doesn't exist yet\n",
    "    if not(path_ss.joinpath(path_dst.name+('.zip')).exists()):\n",
    "        print(f'Zipping zarr-folder of Lake {path.stem.split(\"-\")[-2]}.. ', end='')\n",
    "        shutil.make_archive(path_dst, 'zip', path)\n",
    "        print(f'Finished zipping after {(time.time()-start_time):0.0f} seconds.')\n",
    "\n",
    "print('All zarr-folders converted to .zip files!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
