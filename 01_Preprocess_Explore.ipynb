{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56004dfe",
   "metadata": {},
   "source": [
    "# LakeCREST ESA CCI Lakes v2.0\n",
    "## 1. Preprocessing\n",
    "In this script we will use [**xarray**](https://docs.xarray.dev/en/latest/index.html) and [**dask**](https://dask.org/) to load, prepare and explore the [**ESA CCI Lakes v2.0**](https://climate.esa.int/en/projects/lakes/data/) dataset. Using dask, a free and open-source library for parallel computing, allows us to fully utilize the computing power of our machine and scale our workflow easily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d2cb37",
   "metadata": {},
   "source": [
    "### 1.1 Importing modules\n",
    "First, we import the necessary python modules for the preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a764d406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import time\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "import colorama # for colored text outputs\n",
    "from colorama import Fore, Back, Style"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb01a50",
   "metadata": {},
   "source": [
    "### 1.2 User inputs\n",
    "Here we define the desired lake, variables and paths. More information on available variables and a full list of all lake IDs can be found at the end of the [**D4.3: Product User Guide (PUG)**](https://climate.esa.int/media/documents/CCI-LAKES-0029-PUG_v1.1_signed_CA.pdf) (currently only for v1.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fc4b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes = {\n",
    "    'Michigan':6,\n",
    "    'Superior':2,\n",
    "    'Erie':12,\n",
    "    'Huron':5,\n",
    "    'Ontario':15,\n",
    "    'Kariba':35,\n",
    "    'Garda':505,\n",
    "    'Baikal':8\n",
    "    }\n",
    "lakename = 'Erie'\n",
    "lakeid = lakes.get(lakename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9152919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['lake_surface_water_temperature',\n",
    "             'lswt_quality_level',\n",
    "             'lswt_uncertainty']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3081b709",
   "metadata": {},
   "source": [
    "The paths are initialized as python raw-strings to avoid errors due to escape-sequences and then converted to pathlib.Path objects. The pathlib library allows to use paths within different operating systems and gives us access to many powerful functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c589c5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directory path and filenames\n",
    "path_data = r'D:\\lakecrest\\esa_cci_lp\\v1.1' # CCI data folder path\n",
    "path_mask = r'D:\\lakecrest\\esa_cci_lp\\mask\\ESACCI-LAKES_mask_v1.nc' # mask path\n",
    "path_dask = r'C:\\Users\\Micha\\Desktop\\dask' # Temporary dask workerspace\n",
    "\n",
    "# Get filepaths and convert to pathlib.Path\n",
    "path_data = pathlib.Path(path_data)\n",
    "path_mask = pathlib.Path(path_mask)\n",
    "path_dask = pathlib.Path(path_dask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be84f200",
   "metadata": {},
   "source": [
    "### 1.3 Dask initialization\n",
    "We initialize a local Dask client with our specified number of workers, threads per worker and memory limit (per worker). Calling the client outputs the client adress, so we can access the client over its webinterface. A good starting point for the settings is to set *n_workers* to the number of physical cores and *threads_per_worker* to number of virtual cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ec522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define according to system specs\n",
    "n_workers = 8              # (e.g. number of physical cores)\n",
    "threads_per_worker = 1     # (e.g. virtual cores / n_workers)\n",
    "memory_limit = '4GB'       # (e.g. max memory / n_workers)\n",
    "\n",
    "local_directory = path_dask\n",
    "cluster = LocalCluster(n_workers=n_workers, \n",
    "                       threads_per_worker=threads_per_worker, \n",
    "                       memory_limit=memory_limit,\n",
    "                       local_directory=local_directory\n",
    "                      )\n",
    "client = Client(address=cluster.scheduler_address)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c42490",
   "metadata": {},
   "source": [
    "### 1.4 Specify chunk size\n",
    "We use xarray to load the large multi-file dataset. xarray allows us to initialize and load the entire dataset by only providing the necessary filepaths. Instead of loading the entire dataset (>350GB) to memory, we can make use of xarray's ability to lazy-load chunks of data. This means that only the necessary subset of each individual .nc file will be loaded into memory at the time it is needed. For this we can either pre-define a chunk size or let xarray automatically define a size. Small chunk sizes are prefered since individual lakes are very small in comparison to the size of the global dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0911ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set chunk size based on .nc file dims and a divider\n",
    "#input_lat_sz = 21600\n",
    "#input_lon_sz = 43200\n",
    "#divider=100\n",
    "\n",
    "#chunk_lat_sz = int(input_lat_sz/divider)\n",
    "#chunk_lon_sz = int(input_lon_sz/divider)\n",
    "#chunks={'lat':chunk_lat_sz,\n",
    "#        'lon':chunk_lon_sz,\n",
    "#        'time':1\n",
    "#        }\n",
    "\n",
    "# Alternatively set chunks to 'auto' (dask decides chunk size)\n",
    "chunks='auto'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920c5571",
   "metadata": {},
   "source": [
    "### 1.5 Spatial subsetting\n",
    "Next we setup a function **preprocess** that subsets the daily .nc file to only fetch information about the desired bounding-box based on the bounding coordinates of the desired lake. The preprocess function will be run on every daily .nc file when the multifile-dataset is initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d985622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CCI lakes mask\n",
    "DS_mask = xr.open_dataset(filename_or_obj=path_mask,\n",
    "                          engine='netcdf4',\n",
    "                          decode_cf=False,\n",
    "                          chunks=chunks\n",
    "                          )\n",
    "\n",
    "# Get logical True/False lake mask over full globe\n",
    "mask_full = (DS_mask.CCI_lakeid == lakeid)\n",
    "\n",
    "# Get logical lake mask sliced over ROI\n",
    "mask_roi = DS_mask.CCI_lakeid.where(mask_full, \n",
    "                                   drop=True)\n",
    "\n",
    "# Count number of unmasked lake cells\n",
    "lake_cells = np.count_nonzero(mask_roi)\n",
    "\n",
    "# Get bounds coordinates (in WGS84)\n",
    "lat_min = mask_roi.lat[0].values\n",
    "lat_max = mask_roi.lat[-1].values\n",
    "lon_min = mask_roi.lon[0].values\n",
    "lon_max = mask_roi.lon[-1].values\n",
    "print(f'{Fore.RED}Subsetting to masked ROI with bbox ' \\\n",
    "      f'lat: ({lat_min:0.1f}, {lat_max:0.1f}), ' \\\n",
    "      f'lon: ({lon_min:0.1f}, {lon_max:0.1f}) for Lake {lakename}')\n",
    "\n",
    "# Subset mask to same coords\n",
    "DS_mask_roi = DS_mask.sel(lat=slice(lat_min, lat_max), \n",
    "                          lon=slice(lon_min, lon_max))\n",
    "\n",
    "def preprocess(ds):\n",
    "    '''Keeps only the necessary lat/lon slice when opening .nc files'''\n",
    "    return ds.sel(lat=slice(lat_min, lat_max), \n",
    "                  lon=slice(lon_min, lon_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202ff3a6",
   "metadata": {},
   "source": [
    "### 1.6 Load data as xarray.DataSet\n",
    "Initialize the dataset using xarray's [**xarray.open_mfdataset**](https://docs.xarray.dev/en/latest/generated/xarray.open_mfdataset.html) function. The xarray documentation has an extensive [user-guide](https://xarray.pydata.org/en/stable/user-guide/io.html) with best-practices to load large datasets. We will set ***decode_cf*** to **false** for now and decode the dataset later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b88df3a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use pathlib.Path.rglob function to recursively find all .nc files within folder\n",
    "paths_data = list(path_data.rglob('*fv1.1.nc'))   # search for all files *fv1.1.nc in folders and subfolders in the path\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "DS = xr.open_mfdataset(paths=paths_data,\n",
    "                       combine='by_coords',\n",
    "                       parallel=True,\n",
    "                       engine='netcdf4',\n",
    "                       decode_cf=False,\n",
    "                       decode_times=True,\n",
    "                       preprocess=preprocess,\n",
    "                       chunks=chunks\n",
    "                       )\n",
    "\n",
    "print(f'{Fore.RED}Xarray dataset with variables: {variables} initialized after' \\\n",
    "      f'{(time.time()-start_time):0.1f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5537d7ea",
   "metadata": {},
   "source": [
    "We can get a overview of the xarray.Dataset and it's variables and attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db624099",
   "metadata": {},
   "outputs": [],
   "source": [
    "DS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6421d43",
   "metadata": {},
   "source": [
    "### 1.7 Apply lake-mask\n",
    "Next we apply the lake mask to mask cells of possible other lakes in the same bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1291e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the lake mask with specified CCI_lakeid\n",
    "# use reindex to make sure mask cells are aligned to data cells\n",
    "da_mask = (DS_mask_roi.CCI_lakeid == lakeid).reindex_like(other=DS, \n",
    "                                                          method='nearest')\n",
    "\n",
    "# Get subset with lakemask and xarray.DataArray.where\n",
    "DS_clip = DS.where(cond=da_mask, \n",
    "                   drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e73e080",
   "metadata": {},
   "source": [
    "### 1.8 Decode the data\n",
    "xarray will handle the data-decoding of the NetCDF format with the scaling- and offset-attributes found in the loaded files. We can use [xarray.decode_cf](https://docs.xarray.dev/en/latest/generated/xarray.decode_cf.html) to automatically decode the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290b9da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode data\n",
    "DS_clip = xr.decode_cf(DS_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d72967d",
   "metadata": {},
   "source": [
    "### 1.9 Drop days with no data\n",
    "We can drop days with all-nan values for the variable 'lake_surface_water_temperature' by using the [**xarray.Dataset.dropna**](https://docs.xarray.dev/en/latest/generated/xarray.Dataset.dropna.html) function with the parameter ***how='all'***. Alternatively we can also define a nan-treshhold for the parameter ***tresh*** to discard days with equal or more nan-cells than a treshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d32c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_clip_dropna = DS_clip.dropna(dim='time', \n",
    "                                how='all',\n",
    "                                subset=['lake_surface_water_temperature'])\n",
    "\n",
    "# alternative using threshold (e.g. days with 50% or more nan-cells)\n",
    "#nan_tresh = int(lake_cells*0.5) \n",
    "#DS_clip_dropna = DS_clip.dropna(dim='time',\n",
    "#                                tresh=nan_tresh,\n",
    "#                                subset=['lake_surface_water_temperature'])\n",
    "\n",
    "dropcount = DS_clip.time.size - DS_clip_dropna.time.size\n",
    "print(f'{Fore.RED}Dropped {dropcount} days with all-nan cells (from {DS_clip.time.size} to {DS_clip_dropna.time.size})')\n",
    "\n",
    "# alternative with treshold\n",
    "#print(f'{Fore.RED}Dropped {dropcount} days with more than {nan_tresh/lake_cells*100:0.0f}% cells (from {DS.time.size} to {DS_dropna.time.size})')\n",
    "\n",
    "DS_clip_dropna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7194c936",
   "metadata": {},
   "source": [
    "### 1.10 Mask LSWT by quality flag\n",
    "We convert the [xarray.Dataset](https://docs.xarray.dev/en/latest/generated/xarray.Dataset.html) to a [xarray.Dataarray](https://docs.xarray.dev/en/latest/generated/xarray.DataArray.html) containing only the variable 'lake_surface_water_temperature' and convert the value from °K to °C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e5e20a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lswt_celsius = (DS_clip_dropna.lake_surface_water_temperature-273.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249cacd5",
   "metadata": {},
   "source": [
    "Using the variable 'lswt_quality_level' we can mask out cells that don't fulfill the necessary quality criteria that ranges from 0 to 5:\n",
    "\n",
    "- 0: unprocessed \n",
    "- 1: bad \n",
    "- 2: marginal \n",
    "- 3: intermediate\n",
    "- 4: good \n",
    "- 5: best\n",
    "\n",
    "Within the [D4.1: Product Validation and Intercomparison Report](https://climate.esa.int/media/documents/CCI-LAKES-0031-PVIR_v1.4.pdf) the authors of the ESA CCI Lakes product recommend to limit the use of LSWT for lake-climate applications to quality flags 4-5 (good and best)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1ec035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use quality flag variable 'lswt_quality_level' to mask out all cells except good and best\n",
    "qmask = (DS_clip_dropna['lswt_quality_level']>3)\n",
    "lswt_celsius = lswt_celsius.where(cond=qmask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ee2045",
   "metadata": {},
   "source": [
    "### 1.11 Export subset\n",
    "If necessary, the subset with the masked data can be exported using [**xarray.Dataset.to_netcdf**](https://xarray.pydata.org/en/stable/generated/xarray.Dataset.to_netcdf.html). We can get the encoding settings (e.g. compression, fillvalue, scale-factor, offset) for the NetCDF export from the previously loaded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b44b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dst = f'D:\\lakecrest\\output\\ESACCI-LAKES-L3S-LK_PRODUCTS-MERGED-{lakename}-fv2.0.nc'\n",
    "path_dst = pathlib.Path(path_dst)\n",
    "\n",
    "# Get encoding settings from DS.encoding and DS.attrs to ensure the encoding setting are identical\n",
    "DS_enc = {}\n",
    "encoding_enc = ['zlib', 'shuffle', 'complevel', \n",
    "              'fletcher32', 'contiguous', 'dtype']\n",
    "attr_enc = ['_FillValue', 'scale_factor', 'add_offset']\n",
    "for var in variables:\n",
    "    DS_enc_encoding = DS.get(var).encoding\n",
    "    DS_enc_fromenc = {k:v for k, v in DS_enc_encoding.items() \\\n",
    "                      if k in encoding_enc}\n",
    "    DS_enc_attrs = DS.get(var).attrs\n",
    "    DS_enc_fromattrs = {k:v for k, v in DS_enc_attrs.items() \\\n",
    "                        if (k in attr_enc and hasattr(DS.get(var), k))}\n",
    "    DS_enc[var] = {**DS_enc_fromenc, **DS_enc_fromattrs}\n",
    "print(f'{Fore.RED}DS encoding:', DS_enc)\n",
    "\n",
    "# Set time slice to get a temporal subset\n",
    "\n",
    "timeslice = slice('2019-01-01', '2020-01-01)\n",
    "\n",
    "# Set starting time for timer\n",
    "start_time = time.time() \n",
    "\n",
    "export = lswt_celsius.sel(time=timeslice)\n",
    "                  \n",
    "export.to_netcdf(path=path_dst,\n",
    "                       mode='w',\n",
    "                       engine='netcdf4',\n",
    "                       encoding=DS_enc\n",
    "                      )\n",
    "\n",
    "print(f'{Fore.RED}ESA CCI Lakes v2.0 subset of Lake {lakename} ' \\\n",
    "      f'exported after {(time.time()-start_time):0.1f} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db29714",
   "metadata": {},
   "source": [
    "## 2. Explore dataset\n",
    "### 2.1 LSWT timeseries animation\n",
    "Create an interactive timeseries animation that displays the temperature data in a cartographic reference system using [Holoviews](https://holoviews.org/) and [Cartopy](https://scitools.org.uk/cartopy/docs/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a387c092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.xarray\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "crs = ccrs.PlateCarree(central_longitude=0, globe=None)\n",
    "lswt_celsius.hvplot(\n",
    "    geo=True, tiles='CartoLight',\n",
    "    groupby=\"time\",  # adds a widget for time\n",
    "    clim=(0, 25),  # sets colormap limits\n",
    "    crs=crs,\n",
    "    cmap='jet',\n",
    "    widget_type=\"scrubber\",\n",
    "    widget_location=\"bottom\",\n",
    "    width=300\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a8eafa",
   "metadata": {},
   "source": [
    "### 2.2 Mean LSWT plot\n",
    "We can use [**xarray.DataArray.mean**](https://docs.xarray.dev/en/latest/generated/xarray.DataArray.mean.html) with the parameters ***dim*** and ***skipna*** to plot the spatially aggregated daily LSWT mean over time. An extensive documentation of the plotting functionality compatible with xarray datatypes can be found at https://docs.xarray.dev/en/latest/user-guide/plotting.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3457f341",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeslice = slice('2006-01-01', '2007-12-31')\n",
    "\n",
    "lswt_celsius_mean = lswt_celsius.mean(dim=[\"lat\", \"lon\"], skipna=True)\n",
    "lswt_celsius_mean.sel(time=timeslice).plot.line(\"b-^\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d7aaee",
   "metadata": {},
   "source": [
    "### 2.3 Visualize data availability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec84caaf",
   "metadata": {},
   "source": [
    "As we can see in the mean plots the data availability changes in accordance with sensor mission timeline. We can use the time dimension to calculate the yearly average coverage frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e066825c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48989d9b",
   "metadata": {},
   "source": [
    "### 2.4 Compute linear trend\n",
    "We can compute the linear trend for each lake cell using the [xarray.DataArray.polyfit](https://docs.xarray.dev/en/stable/generated/xarray.DataArray.polyfit.html) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cc77a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_start = \"1993-01-01\"\n",
    "date_end = \"2018-12-31\"\n",
    "lswt_celsius_subset = lswt_celsius.loc[dict(time=slice(date_start, date_end))].persist()\n",
    "polyfit_results = lswt_celsius_subset.polyfit(dim='time', \n",
    "                                              deg=1, \n",
    "                                              skipna=True)\n",
    "ns_per_year = 3.154e+16\n",
    "ns_per_decade = 3.154e+17\n",
    "trend_coef = polyfit_results.polyfit_coefficients.sel(degree=1)\n",
    "trend_coef_perDec = trend_coef*ns_per_decade # convert °C/ns to °C/10y\n",
    "trend_coef_perDec.load() # load results into memory for faster plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66afaa9",
   "metadata": {},
   "source": [
    "Plot the data using HoloViews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db110ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vmax = np.nanpercentile(trend_coef_perDec, 95)\n",
    "vmax = math.ceil(vmax*10)/10\n",
    "\n",
    "trend_coef_perDec.hvplot(\n",
    "    clabel='trend (°C/10y)', \n",
    "    label=f'Lake {lakename}, Linear trend',\n",
    "    geo=True, \n",
    "    tiles='StamenTerrainRetina', # plot backgroundmap\n",
    "    #features={'rivers':'10m'},\n",
    "    cmap='bwr',\n",
    "    clim=(-vmax, vmax),\n",
    "    crs=crs,\n",
    "    #width=300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8e5957",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = math.ceil(vmax*10)/10\n",
    "steps = int(stop/0.1)*2+1\n",
    "levels = np.linspace(-stop, stop, steps)\n",
    "\n",
    "trend_coef_perDec.hvplot.contourf(\n",
    "    clabel='trend (°C/10y)', \n",
    "    label=f'Lake {lakename}, Linear trend',\n",
    "    geo=True, \n",
    "    tiles='StamenTerrainRetina', # plot backgroundmap\n",
    "    #features={'rivers':'10m'},\n",
    "    levels=levels,  \n",
    "    cmap='bwr',\n",
    "    crs=crs,\n",
    "    #width=300\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933ea6b5",
   "metadata": {},
   "source": [
    "Calculate the mean trend (°C/10y) over entire lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cd682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmean(trend_coef_perDec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ff555d",
   "metadata": {},
   "source": [
    "Plot with matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa40732",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_proj = ccrs.PlateCarree()\n",
    "data_proj = ccrs.PlateCarree()\n",
    "\n",
    "# create figure\n",
    "fig = plt.figure(dpi=150)\n",
    "ax = fig.add_subplot(111, projection=map_proj)\n",
    "\n",
    "# plot contouplot\n",
    "trend_plot = ax.contourf(polyfit_results.lon.values,\n",
    "                         polyfit_results.lat.values,\n",
    "                         polyfit_results.polyfit_coefficients.sel(degree=1)*3.154e+17, # convert from °C/ns to °C/10y\n",
    "                         cmap='bwr',\n",
    "                         levels=np.linspace(-1, 1, 11),\n",
    "                         transform=data_proj,\n",
    "                         extend='both')\n",
    "\n",
    "# add colorbar\n",
    "axpos = ax.get_position()\n",
    "cbar_ax = fig.add_axes([axpos.x1+0.03,axpos.y0,0.03,axpos.height])\n",
    "cbar = fig.colorbar(trend_plot, cax=cbar_ax)\n",
    "cbar.ax.tick_params(labelsize=12)\n",
    "cbar.set_label('LSWT trend (°C/10y)', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7f475c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
